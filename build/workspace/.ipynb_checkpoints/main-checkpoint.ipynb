{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7876809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import json\n",
    "from airflow.providers.postgres.hooks.postgres import PostgresHook      #Устанавливает соединение с postgres\n",
    "from airflow.providers.amazon.aws.hooks.s3 import S3Hook                #Устанавливает соединение с S3\n",
    "from airflow import DAG, models\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.decorators import dag, task\n",
    "import logging\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1c3a902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_s3(data, key, bucket='data'):  #Функция загружает данные в S3, data-файл для загрузки, bucket - имя бакета, key - путь и название файла\n",
    "    logging.info('Создание подключения к S3')\n",
    "    hook = S3Hook(aws_conn_id='minio')    #Установка подключения к S3\n",
    "    \n",
    "    try:\n",
    "        if not hook.check_for_bucket(bucket):           #Проверяет есть ли бакет с таким именем\n",
    "            hook.create_bucket(bucket_name=bucket)\n",
    "        logging.info('Подключение установлено')\n",
    "    except Exception as e:                              #Логирование ошибки\n",
    "        logging.error(f'Ошибка подключения к minio, {e}')\n",
    "                \n",
    "    logging.info('Загрузка данных в S3')\n",
    "    \n",
    "    \n",
    "    hook.load_string(                   #Загружает строку в S3\n",
    "        string_data=data,           #Сериализованные в строку данные\n",
    "        key=key,                    #Путь до файла в заданном бакете\n",
    "        bucket_name=bucket,         #Название бакета\n",
    "        replace=True\n",
    "    )\n",
    "    \n",
    "    logging.info('Загрузка завершена успешно')\n",
    "    \n",
    "    return {'bucket': bucket, 'key': key}       #Возвращает путь до файла и название бакета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bf0109",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'start_date': datetime(2024, 1, 1),\n",
    "    'retries': 2,\n",
    "    'retry_delay': timedelta(minutes=1)\n",
    "}\n",
    "with DAG(dag_id='ETL_crypto_data', default_args=default_args, schedule_interval='hourly', catchup=False, tags=['cost']) as dag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaeca91",
   "metadata": {},
   "outputs": [],
   "source": [
    "    @task\n",
    "    def conn_to_spark():\n",
    "        spark = SparkSession \\\n",
    "                    .builder \\\n",
    "                    .appName(\"crypto_analyzer_local\") \\\n",
    "                    .master(\"spark://spark-master:7077\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "        try:\n",
    "            test = spark.range(100)\n",
    "            test.count()\n",
    "            print('Соединение с кластером Spark установлено успешно')\n",
    "        except Exception as e:\n",
    "            print(f'Ошибка соединения {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfe9c781",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Ошибка преобразование данных\n"
     ]
    }
   ],
   "source": [
    "    @task\n",
    "    def extract_transform():\n",
    "        url = \"https://api.bybit.com/v5/market/kline\"\n",
    "        params = {\n",
    "            \"category\": \"spot\",\n",
    "            \"symbol\": \"BTCUSDT\",\n",
    "            \"interval\": \"15\",\n",
    "            \"limit\": 5\n",
    "        }\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            data_json = response.json()\n",
    "            try:\n",
    "                df = spark.createDataFrame(data_json['result']['list'], schema=['start_time', 'open', 'high', 'low', 'close', 'volume', 'usdt_volume']).withColumn('open', F.col('open').cast('float'))\\\n",
    "                .withColumn('close', F.col('close').cast('float'))\\\n",
    "                .withColumn('high', F.col('high').cast('float'))\\\n",
    "                .withColumn('low', F.col('low').cast('float'))\\\n",
    "                .withColumn('volume', F.col('volume').cast('float'))\\\n",
    "                .withColumn('usdt_volume', F.col('usdt_volume').cast('float'))\\\n",
    "                .withColumn('start_time', F.to_timestamp(F.col('start_time') / 1000))\\\n",
    "                .orderBy(F.col('start_time'))\n",
    "                logging.info('Формат данных успешно преобразован, начинаю загрузку в S3')\n",
    "                df = df.toJSON()\n",
    "                upload_to_s3(df, f'transist_data/data_{datetime.date(datetime.now())}_')\n",
    "            except:\n",
    "                logging.error('Ошибка преобразование данных')\n",
    "        else:\n",
    "            logging.error('Ошибка соединения с API')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafa06dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "    conn_to_spark()\n",
    "    extract_transform()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
